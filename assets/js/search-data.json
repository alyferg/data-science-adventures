{
  
    
        "post0": {
            "title": "All at odds - Part 2",
            "content": "In the last post, we analysed the starting odds for games that ended as Home wins, Away wins, and Draws. We noticed something in the charts that required further investigation. When we looked at the starting odds for games that ended as a Home win the starting odds for the game, in the vast majority of cases, had the Home win odds shorter (lower) than Away win and Draw odds. In other words, it looked as if the market was very good at predicting Home wins. Similar analysis of Away wins and Draws did not yield matching results. Now, we need to investigate that finding further. . Let&#39;s have a quick recap of the findings from the last time, so we can reference it here. Without explaining in detail, here is the code we used to produce the Boxplot for Home wins. For a detailed explanation then you can reference Part 1 of this series. . # from Part 1 of this series # load dependencies and data import pandas as pd import numpy as np import glob import matplotlib.pyplot as plt import seaborn as sns from pandas.plotting import scatter_matrix #load all csv files and append to a single dataframe path = &#39;../../../GitHub/England-EPL&#39; all_files = glob.glob(path + &quot;/*.csv&quot;) li = [] for filename in all_files: df = pd.read_csv(filename, index_col=None, header=0) li.append(df) footdata = pd.concat(li, axis=0, ignore_index=True) footdata.rename(columns={&#39;B365H&#39;: &#39;Home odds&#39;, &#39;B365A&#39;: &#39;Away odds&#39;, &#39;B365D&#39;: &#39;Draw odds&#39;}, inplace=True, errors=&#39;raise&#39;) # to make the dataframe more manageable and readable let&#39;s select only the columns we want to analyse footdata_odds = footdata[[&#39;FTR&#39;,&#39;Home odds&#39;,&#39;Away odds&#39;,&#39;Draw odds&#39;]] #create a table with a subset of the data for Home win, Away win and Draw footdata_odds_h = footdata_odds.loc[footdata_odds[&#39;FTR&#39;] == &quot;H&quot;] footdata_odds_a = footdata_odds.loc[footdata_odds[&#39;FTR&#39;] == &quot;A&quot;] footdata_odds_d = footdata_odds.loc[footdata_odds[&#39;FTR&#39;] == &quot;D&quot;] #plot the boxplot for games that ended a home win. Nice and big, so we can get a good look at the data. footdata_odds_h.plot(kind=&#39;box&#39;, figsize=(12,15),title=(&#39;Starting odds for home wins&#39;),grid=&#39;TRUE&#39;) . &lt;AxesSubplot:title={&#39;center&#39;:&#39;Starting odds for home wins&#39;}&gt; . The vast majority of games show that the starting odds of the Home win are less than the starting odds for Away win and Draw. As a general rule, the odds for the Draw tend to be higher than at least one, if not both of the other odds. More about this later. . We need to add a word of caution at this point. What we are seeing is the starting odds for matches that ended as a Home win. Of course, at the start of the game, we don&#39;t know the outcome. So the question that we need to answer is &quot;what is the probability of a Home win when the Home win odds are less than Away odds or Draw odds&quot; . The ideal tool for answering the question &quot;What is the probability of X given Y&quot; is Bayes Theorem. That is exactly what Bayes does and it will help us here to very easily decide if there is a direct link between the outcome given the odds at the start. I also have to acknowledge the wonderful work of reference Think Bayes by Allen B. Downey was in helping me to understand the concept and answer this question. Anyone who has read the book may see some familiar code used here. . We have already loaded the data we are to use, let&#39;s have a quick look at the main table. Then we will drop data columns with the exception of the result and starting odds columns. . footdata . Div Date HomeTeam AwayTeam FTHG FTAG FTR HTHG HTAG HTR ... AvgC&lt;2.5 AHCh B365CAHH B365CAHA PCAHH PCAHA MaxCAHH MaxCAHA AvgCAHH AvgCAHA . 0 E0 | 14/08/10 | Aston Villa | West Ham | 3.0 | 0.0 | H | 2.0 | 0.0 | H | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 1 E0 | 14/08/10 | Blackburn | Everton | 1.0 | 0.0 | H | 1.0 | 0.0 | H | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2 E0 | 14/08/10 | Bolton | Fulham | 0.0 | 0.0 | D | 0.0 | 0.0 | D | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 3 E0 | 14/08/10 | Chelsea | West Brom | 6.0 | 0.0 | H | 2.0 | 0.0 | H | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 4 E0 | 14/08/10 | Sunderland | Birmingham | 2.0 | 2.0 | D | 1.0 | 0.0 | H | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 4418 E0 | 23/05/2021 | Liverpool | Crystal Palace | 2.0 | 0.0 | H | 1.0 | 0.0 | H | ... | 3.49 | -2.25 | 1.86 | 2.04 | 1.88 | 2.03 | 1.98 | 2.14 | 1.88 | 2.00 | . 4419 E0 | 23/05/2021 | Man City | Everton | 5.0 | 0.0 | H | 2.0 | 0.0 | H | ... | 2.77 | -1.75 | 2.01 | 1.89 | 1.99 | 1.89 | 2.20 | 2.00 | 2.03 | 1.85 | . 4420 E0 | 23/05/2021 | Sheffield United | Burnley | 1.0 | 0.0 | H | 1.0 | 0.0 | H | ... | 2.05 | 0.00 | 2.04 | 1.86 | 2.05 | 1.86 | 2.17 | 1.90 | 2.03 | 1.84 | . 4421 E0 | 23/05/2021 | West Ham | Southampton | 3.0 | 0.0 | H | 2.0 | 0.0 | H | ... | 2.14 | -0.75 | 2.00 | 1.90 | 2.02 | 1.91 | 2.06 | 2.01 | 1.99 | 1.89 | . 4422 E0 | 23/05/2021 | Wolves | Man United | 1.0 | 2.0 | A | 1.0 | 2.0 | A | ... | 1.62 | -0.25 | 2.04 | 1.86 | 2.10 | 1.84 | 2.10 | 1.94 | 2.00 | 1.88 | . 4423 rows × 139 columns . # to make the dataframe more manageable and readable let&#39;s select only the columns we want to analyse footdata_odds = footdata[[&#39;FTR&#39;,&#39;Home odds&#39;,&#39;Away odds&#39;,&#39;Draw odds&#39;]] footdata_odds . FTR Home odds Away odds Draw odds . 0 H | 2.00 | 4.0 | 3.30 | . 1 H | 2.88 | 2.5 | 3.25 | . 2 D | 2.20 | 3.4 | 3.30 | . 3 H | 1.17 | 17.0 | 7.00 | . 4 D | 2.10 | 3.6 | 3.30 | . ... ... | ... | ... | ... | . 4418 H | 1.14 | 15.0 | 8.50 | . 4419 H | 1.44 | 6.5 | 5.00 | . 4420 H | 2.80 | 2.4 | 3.60 | . 4421 H | 1.65 | 5.0 | 4.00 | . 4422 A | 2.50 | 2.7 | 3.50 | . 4423 rows × 4 columns . The following code creates 2 functions to call as we prepare to analyse the probabilities. The first allows us to calculate the probability of a particular outcome occurring while the second gives us a conditional probability. It is the latter we wish to know but to get there we need to extract the right data. . def prob(A): &quot;&quot;&quot;Computes the probability of a proposition, A.&quot;&quot;&quot; return A.mean() . def conditional(proposition, given): &quot;&quot;&quot;Probability of A conditioned on given.&quot;&quot;&quot; return prob(proposition[given]) . Next, we prepare to make our comparisons. We will create three tables &#39;home&#39;, &#39;away&#39;, and &#39;draw&#39;. Each of these will contain 4422 rows and a Boolean representing Home wins, Away wins, and Draws. Using Pandas to create these tables, each stores the Boolean as 0 for FALSE and 1 for TRUE. This then enables us to &#39;sum&#39; the total True values in each. We can also use the &#39;mean&#39; function to calculate the number of TRUE entries as a proportion of the total i.e. the probability of a particular outcome is the series. . home = (footdata[&#39;FTR&#39;] == &#39;H&#39;) home.head() . 0 True 1 True 2 False 3 True 4 False Name: FTR, dtype: bool . away = (footdata[&#39;FTR&#39;] == &#39;A&#39;) away.head() . 0 False 1 False 2 False 3 False 4 False Name: FTR, dtype: bool . draw = (footdata[&#39;FTR&#39;] == &#39;D&#39;) draw.head() . 0 False 1 False 2 True 3 False 4 True Name: FTR, dtype: bool . home.sum(), away.sum() ,draw.sum() . (1967, 1359, 1096) . home.mean(), away.mean() , draw.mean() . (0.44472077775265656, 0.3072575175220439, 0.24779561383676238) . Now if we run these tables through our function &#39;prob&#39; created earlier, then we can see the probability of the outcome. These are probabilities, i.e. between 0 and 1. So for Home wins 0.44472077775265656 equates to approx 44.5% of all matches ending as a Home win. Similarly, 30.7% Away wins and 24.8% draws. We know from these figures compared to the &#39;sum&#39; and &#39;mean&#39; functions above that the &#39;prob&#39; function works. . prob(home), prob(away), prob(draw) . (0.44472077775265656, 0.3072575175220439, 0.24779561383676238) . Similarly, we now create tables for odds where Home odds are lower than Away odds or vice versa. Here, we are ignoring the odds for the Draw. Simply put the odds for the Draw are never lower than those for a result either at home or away. Creating a table for situations where Draw odds were lower would only result in an empty table. This reflects the difficulty in predicting draws in football. . lower_odds = (footdata[&#39;Home odds&#39;] &lt; (footdata[&#39;Away odds&#39;])) &amp; (footdata[&#39;Home odds&#39;] &lt; (footdata[&#39;Draw odds&#39;])) lower_odds.sum() . 3049 . higher_odds = (footdata[&#39;Away odds&#39;] &lt; (footdata[&#39;Home odds&#39;])) &amp; (footdata[&#39;Away odds&#39;] &lt; (footdata[&#39;Draw odds&#39;])) higher_odds.sum() . 1347 . prob(lower_odds), prob(higher_odds) . (0.6893511191498982, 0.30454442685959754) . At last, we are all ready to go. We see from the above that around 70% of the time Home win odds are lower than away win odds, which you would expect to be the case. Exciting, at last, we can find the answer to our question &quot;what is the probability of a Home win when the Home win odds are less than Away odds or Draw odds&quot; . conditional(home, given=lower_odds) . 0.5388652017054772 . I have to be honest and say that was rather an anticlimax. Where the Home win odds are below the odds for an Away win, the outcome is a home win only 53.8% of the time. This seems really low given the chart we saw at the start. It looked like odds for Home wins were always below the others, so what happened? . The outcome can be explained with the following two main reasons. . The impact of the Draw. We know that 24.7% of games end as a draw, but that the odds of a Draw are always above those of Home and Away wins. Therefore, the best outcome for Home wins where Home win odds are lower is around 75% of games as around 25% will end up as Draws. . | The other 21% of games? Well, that&#39;s just the nature of football. Players, teams, referees have an off day. Balls hitting the woodwork and not going into the goal, deflections of shots past a goalkeeper, etc etc etc. That&#39;s the randomness of football. . | If we were to disregard the draw, then we would be 78% accurate, but the draw is part of the game. . Let&#39;s just check the figures for matches where the outcome is a Draw and see how that compares to the starting odds. . conditional(draw, higher_odds), conditional(draw, lower_odds) . (0.2457312546399406, 0.24762217120367333) . That&#39;s why football is such an exciting game. Whether odds for a Home win or an Away win are higher, the draw ratio is always around 24.6/24.7% almost the same. These figures are based on a sample size of 4423 games. As that number increases the figures would tend to merge. . Of course in our analysis, we have treated every team the same. We have not looked at strong teams against weak teams. We wanted to see the outcomes based purely on the starting odds. Nor have we considered the relative strength of the odds. That&#39;s for the future when we consider more than one variable at a time. . For the sake of completeness let&#39;s check the various outcomes where the Home odds were lower than the away odds. . conditional(home, given=lower_odds) , conditional(away, given=lower_odds) , conditional(draw, given=lower_odds) . (0.5388652017054772, 0.21351262709084945, 0.24762217120367333) . There we can see it. The 21.3% of games end as Away wins where Home odds were lower while 24.7% end as draws. What about the situation where the Away odds are higher, i.e. the market expects an Away win. . Similarly, when Away odds are higher. . conditional(home, given=higher_odds) , conditional(away, given=higher_odds), conditional(draw, given=higher_odds) . (0.23088344469190794, 0.5233853006681515, 0.2457312546399406) . Amazing, almost identical figures. 52.3% accurate with 24.6% draws. That leaves 23% as the random element, being very close to the figures we found in the case of Home win odds being lower. Again, as the sample size increases towards infinity, these two figures will tend to merge. . Out of curiosity what happens if we turn the probability prediction on its head. Given a Home win, what is the probability of getting Home odds lower? . conditional(lower_odds, given=home), conditional(lower_odds, given=away), conditional(lower_odds, given=draw) . (0.835282155566853, 0.47902869757174393, 0.6888686131386861) . That would have been ideal. Where we have a home win nearly 84% of games have lower starting odds for the Home win. Regrettably, we don&#39;t know the outcome before the game starts. Shame. This reflects the outcome that we saw in the Boxplot in the introduction above. It looked useful but has proven not to be so. Finally, for the sake of being complete let&#39;s compare the lower Away odds for various outcomes. . conditional(higher_odds, given=home), conditional(higher_odds, given=away), conditional(higher_odds, given=draw) . (0.15810879511947126, 0.5187637969094923, 0.30200729927007297) . Conclusion . We asked the question &quot;what is the probability of a Home win when the Home win odds are less than Away odds or Draw odds&quot;. We were able to calculate the answer, though it did not help us much. The 53% probability of a Home win when Home win odds are the lowest is not enough to be the foundation for a winning strategy on its own. We would need further variables to improve the accuracy. Still, remember that figure as it may prove useful in the future. . Home wins have starting Home odds lower than Away odds 83% of the time. This is really useful information. However, as we don&#39;t know the outcome at the start, it does not help us at all. . In a way, this is a cautionary tale in the world of Data Science. There are often different ways to look at things. We need to check, double-check, and sanity check to prevent ourselves from coming to false or unhelpful conclusions. If you want to learn more about Bayes, then I suggest having a look at Allen Downeys book Think Bayes, which is available on Github. There are a few interesting exercises there to help give a different perspective and show how easy it is to make assumptions and/or come to the wrong conclusion. . Still, I have enjoyed playing with Bayes. So I think, that for Part 3 of this series we will have another Bayes study, this time on a different area. .",
            "url": "https://alyferg.github.io/data-science-adventures/2021/12/21/All-at-odds-Part-2.html",
            "relUrl": "/2021/12/21/All-at-odds-Part-2.html",
            "date": " • Dec 21, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "All at odds - part 1",
            "content": "It won&#39;t come as a shock to anyone that this is not the only blog on this particular subject. There are many blogs YouTube channels, etc. on the subject of data Science and specifically data Science in Sport and Sports betting. It is interesting when looking at these other resources how different people take different routes, one assumes or at least talks, with the same destination in mind. To date, I have not seen another blog that uses the same starting point as I am about to, though there is a very good chance that there is one out there somewhere. . Historically, the only place to go to put a bet on sports was a bookmaker, in fact until the late 1990s that would have meant visiting a shop for most people. At a bookmaker, you could place a back bet on a particular outcome and that was it. Now, when you visit a bookmaker you have a huge range of different options available to you. Especially in sports like football all where you can bet on everything from the outcome of the match to the number of corners a particular side will have. That is not the only thing that changed in Sports betting because around 2000 betting exchanges were introduced. Exchanges are effectively peer-to-peer betting services where the exchange provides the infrastructure to allow people to exchange bets. Your exchange rate could be against another individual, a syndicate, an organisation, or even a bookmaker, you just don&#39;t know as you&#39;ll never see the other side of the bet. The other thing that changed with the advent of betting exchanges was that you can now bet on something not happening, i.e. you can bet against a particular team winning rather than just backing them to win. . My journey into the analysis of Sports Betting will begin with looking at the accuracy of the odds at the beginning of a particular event. The odds offered by individual bookmakers tend to match closely those offered by other bookmakers and those offered at the exchanges. This needs to be the case otherwise there would be the potential for bettors to exploit differences in prices between different bookmakers and exchanges. That technique is known as arbitrage betting. The price that is available just before the commencement of a particular event it&#39;s probably as close as the real probability of a particular outcome for that event, for two very good reasons. Where the price is being set by pressure from an exchange, then we have the &quot;wisdom of the crowd&quot; coming into play. There could be thousands of individuals looking for value that will drive the price to a balance point of maximum value. The second reason, where the price is being driven by pressure from bookmakers then their experience will come into play. This knowledge over many many years, plus, no doubt, the data scientists that they employ to calculate real probability, leads to the price at that particular point in time will be as close to real as possible. . Therefore, my opening question is . . . . How accurate are the starting odds set for a market in a particular event? . I will be using the starting prices from a bookmaker for this particular exercise, but that&#39;s only due to the availability of the data and is not a reflection of the perceived accuracy of that data. Should data from betting exchanges for the same event start to be collected then there will be very little difference between not and the data we have used. The first analysis will be to look at match odds data, i.e. Home win away win what draws in Premier League football matches between 2010 and 2021. . The analysis is undertaken with the data used in the previous set downloaded from . FootballData.co.uk . First with the English Premiership between 2010 and 2021, then we can look at other leagues. . # load dependencies and data import pandas as pd import numpy as np import glob import matplotlib.pyplot as plt import seaborn as sns from pandas.plotting import scatter_matrix #load all csv files and append to a single dataframe path = &#39;../../../GitHub/England-EPL&#39; all_files = glob.glob(path + &quot;/*.csv&quot;) li = [] for filename in all_files: df = pd.read_csv(filename, index_col=None, header=0) li.append(df) footdata = pd.concat(li, axis=0, ignore_index=True) footdata.rename(columns={&#39;B365H&#39;: &#39;Home odds&#39;, &#39;B365A&#39;: &#39;Away odds&#39;, &#39;B365D&#39;: &#39;Draw odds&#39;}, inplace=True, errors=&#39;raise&#39;) #check that the data is loaded and see how it looks footdata . Div Date HomeTeam AwayTeam FTHG FTAG FTR HTHG HTAG HTR ... AvgC&lt;2.5 AHCh B365CAHH B365CAHA PCAHH PCAHA MaxCAHH MaxCAHA AvgCAHH AvgCAHA . 0 E0 | 14/08/10 | Aston Villa | West Ham | 3.0 | 0.0 | H | 2.0 | 0.0 | H | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 1 E0 | 14/08/10 | Blackburn | Everton | 1.0 | 0.0 | H | 1.0 | 0.0 | H | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2 E0 | 14/08/10 | Bolton | Fulham | 0.0 | 0.0 | D | 0.0 | 0.0 | D | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 3 E0 | 14/08/10 | Chelsea | West Brom | 6.0 | 0.0 | H | 2.0 | 0.0 | H | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 4 E0 | 14/08/10 | Sunderland | Birmingham | 2.0 | 2.0 | D | 1.0 | 0.0 | H | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 4418 E0 | 23/05/2021 | Liverpool | Crystal Palace | 2.0 | 0.0 | H | 1.0 | 0.0 | H | ... | 3.49 | -2.25 | 1.86 | 2.04 | 1.88 | 2.03 | 1.98 | 2.14 | 1.88 | 2.00 | . 4419 E0 | 23/05/2021 | Man City | Everton | 5.0 | 0.0 | H | 2.0 | 0.0 | H | ... | 2.77 | -1.75 | 2.01 | 1.89 | 1.99 | 1.89 | 2.20 | 2.00 | 2.03 | 1.85 | . 4420 E0 | 23/05/2021 | Sheffield United | Burnley | 1.0 | 0.0 | H | 1.0 | 0.0 | H | ... | 2.05 | 0.00 | 2.04 | 1.86 | 2.05 | 1.86 | 2.17 | 1.90 | 2.03 | 1.84 | . 4421 E0 | 23/05/2021 | West Ham | Southampton | 3.0 | 0.0 | H | 2.0 | 0.0 | H | ... | 2.14 | -0.75 | 2.00 | 1.90 | 2.02 | 1.91 | 2.06 | 2.01 | 1.99 | 1.89 | . 4422 E0 | 23/05/2021 | Wolves | Man United | 1.0 | 2.0 | A | 1.0 | 2.0 | A | ... | 1.62 | -0.25 | 2.04 | 1.86 | 2.10 | 1.84 | 2.10 | 1.94 | 2.00 | 1.88 | . 4423 rows × 139 columns . # to make the dataframe more manageable and readable let&#39;s select only the columns we want to analyse footdata_odds = footdata[[&#39;FTR&#39;,&#39;Home odds&#39;,&#39;Away odds&#39;,&#39;Draw odds&#39;]] footdata_odds . FTR Home odds Away odds Draw odds . 0 H | 2.00 | 4.0 | 3.30 | . 1 H | 2.88 | 2.5 | 3.25 | . 2 D | 2.20 | 3.4 | 3.30 | . 3 H | 1.17 | 17.0 | 7.00 | . 4 D | 2.10 | 3.6 | 3.30 | . ... ... | ... | ... | ... | . 4418 H | 1.14 | 15.0 | 8.50 | . 4419 H | 1.44 | 6.5 | 5.00 | . 4420 H | 2.80 | 2.4 | 3.60 | . 4421 H | 1.65 | 5.0 | 4.00 | . 4422 A | 2.50 | 2.7 | 3.50 | . 4423 rows × 4 columns . # there are some NaN values in the dataframe. Here are the counts per column footdata_odds.isna().sum() . FTR 1 Home odds 1 Away odds 1 Draw odds 1 dtype: int64 . We will initially look at this data via a Boxplot. Boxplot is similar to a candle chart for anyone who has done any technical charting. What a Boxplot does is give us a Box showing where the majority of the data is while showing outliers as lines and dots. In this way, we can see where the majority of the data sits. . The main Box in a Boxplot shows all data from the Q1 or 25th percentile to the Q3 or 75th percentile. Using a Boxplot we can see how closely grouped the data or in this case the starting odds are. The lines and dots above and below these levels will show how spread out the rest of the data is. With this technique, we will be able to get a good picture of where the starting odds are in relation to each outcome and thus an idea of the accuracy of the odds. . We will look at 3 Boxplots - one for each of the 3 possible outcomes. To do this we will split the odds dataset we just created into 3. . #create a table with a subset of the data for Home win, Away win and Draw footdata_odds_h = footdata_odds.loc[footdata_odds[&#39;FTR&#39;] == &quot;H&quot;] footdata_odds_a = footdata_odds.loc[footdata_odds[&#39;FTR&#39;] == &quot;A&quot;] footdata_odds_d = footdata_odds.loc[footdata_odds[&#39;FTR&#39;] == &quot;D&quot;] . #plot the boxplot for games that ended a home win. Nice and big, so we can get a good look at the data. footdata_odds_h.plot(kind=&#39;box&#39;, figsize=(12,15),title=(&#39;Starting odds for home wins&#39;),grid=&#39;TRUE&#39;) . &lt;AxesSubplot:title={&#39;center&#39;:&#39;Starting odds for home wins&#39;}&gt; . Now we know what a Boxplot looks like then let&#39;s have a more detailed explanation of what the elements are. Looking first at the box, the horizontal line in the middle represents the 50% point of the data numbers, the median. From the median line to the bottom of the box is 50% to 25% of data points, while the top part of the box is 50% to 75% of data points. The box, i.e. 25% to 75% of data points is known as the interquartile range. . The lines emanating from the top and bottom of the boxes are known as whiskers and represent the top (75%+) and bottom (25%-) of the data points. The bar across the whiskers is set at (1.5 * Interquartile Range) and the dots above and below these are the Outliers. The Outliers will generally cover approx 0.7% of data values, i.e. 0.35% highest and lowest values in the range. . With the above in mind, how to interpret the Boxplot. The obvious feature is that for Home wins the vast majority of Home odds fall below those of the Away win and the Draw. This is the first area of further identification that we have identified. Before we do though, let&#39;s have a look at the same plots for Away wins and Draws as the outcome. . #let&#39;s plot the boxplot for games that ended as an away win. footdata_odds_a.plot(kind=&#39;box&#39;,figsize=(12,15),title=(&#39;Starting odds for away wins&#39;),grid=&#39;TRUE&#39;) . &lt;AxesSubplot:title={&#39;center&#39;:&#39;Starting odds for away wins&#39;}&gt; . #let&#39;In the Boxplots for Away wins and Draws the overlap of odds is much more pronounced than for the Home wins we looked at earlier. It would be fair to say at this stage that for Away wins and Draws odds alone would not be a sufficient distinguishing factor to develop any conclusions from. Another variable would be required to provide clearer results. That leaves us with the further analysis of the Home win odds in relation to Home win outcomes.s plot the boxplot for games that ended a home win. Nice and big, so we can see the outcome. footdata_odds_d.plot(kind=&#39;box&#39;,figsize=(12,15),title=(&#39;Starting odds for draws&#39;),grid=&#39;TRUE&#39;) . &lt;AxesSubplot:title={&#39;center&#39;:&#39;Starting odds for draws&#39;}&gt; . In the Boxplots for Away wins and Draws the overlap of odds is much more pronounced than for the Home wins we looked at earlier. It would be fair to say at this stage that for Away wins and Draws odds alone would not be a sufficient distinguishing factor to develop any conclusions from. Another variable would be required to provide clearer results. That leaves us with the further analysis of the Home win odds in relation to Home win outcomes. . That will be our starting point for part 2 of this series on using starting odds for analysis of outcome. .",
            "url": "https://alyferg.github.io/data-science-adventures/2021/11/24/All-at-odds-Part-1.html",
            "relUrl": "/2021/11/24/All-at-odds-Part-1.html",
            "date": " • Nov 24, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Application",
            "content": "I mentioned in an earlier post that one of the drivers behind my interest in learning Data Science/Machine Learning was that I had an application. While I see this application as a driver to learn and explore the data, it is the motivation rather than the outcome I am seeking. So here I want to briefly run over the application to explain what I want to achieve before beginning the journey in vain. . The application that I wish to develop is around Sports Betting and/or Trading. I don&#39;t want this blog to become a detailed description of Sports Betting. Therefore, if you wish to know more about Betting and Trading Sports then I suggest you search for some of the numerous resources available. Specifically, I am interested in predicting the outcomes of Football (that&#39;s in the British context) matches and to a lesser extent horse races. A tall order. . It will be worth explaining at this point what this journey is not about. It is not about &quot;beating the Bookies,&quot; not about &quot;making money,&quot; not about finding &quot;a Holy Grail,&quot; in Sports Betting. What I hope to do is to explore the data to find patterns and to be able to predict outcomes of Football, for example, with a reasonable degree of accuracy. These predictions can then be used in further investigation. One thing I have learned in Sports Betting is that there is no such thing as a &quot;certainty&quot;. Any individual, team or horse can have an &quot;off&quot; day, be affected by the conditions, be over-confident etc. etc. etc. All of which can lead to an unexpected outcome, so having a shortlist to consider on the day is the sensible way to go. . So that is the Application. Some of the data I will be using is in the public domain, other data is proprietary and cannot be shared. . My intention is to follow various techniques, i.e. Numerical Regression, Logical Regression, Decision Trees, Random Forests, Deep Learning and see where we get. There may be others that pop up on the way. . I have no idea at the moment how long the journey may be and where it will take me. Isn&#39;t that exciting. . As we now know the path we are following then let&#39;s start with a basic analysis of some of the data we will be using. In a Football Match, there are three possible outcomes (unless it is a cup game that does require a winner). What proportion of games ends up with each of the three possible outcomes? The data was downloaded from . FootballData.co.uk . The data is stored in csv files on my computer with 1 file per season. The first cells will load the dependencies and all csv files and append them to a single dataframe. . The data loaded is for English Premier League from 2010 to 2021. . # load dependencies and data import pandas as pd import numpy as np import glob from matplotlib import pyplot as plt import seaborn as sns #load all csv files and append to a single dataframe path = &#39;../../../GitHub/England-EPL&#39; all_files = glob.glob(path + &quot;/*.csv&quot;) li = [] for filename in all_files: df = pd.read_csv(filename, index_col=None, header=0) li.append(df) footdata = pd.concat(li, axis=0, ignore_index=True) #check that the data is loaded and see how it looks footdata . Div Date HomeTeam AwayTeam FTHG FTAG FTR HTHG HTAG HTR ... AvgC&lt;2.5 AHCh B365CAHH B365CAHA PCAHH PCAHA MaxCAHH MaxCAHA AvgCAHH AvgCAHA . 0 E0 | 14/08/10 | Aston Villa | West Ham | 3.0 | 0.0 | H | 2.0 | 0.0 | H | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 1 E0 | 14/08/10 | Blackburn | Everton | 1.0 | 0.0 | H | 1.0 | 0.0 | H | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2 E0 | 14/08/10 | Bolton | Fulham | 0.0 | 0.0 | D | 0.0 | 0.0 | D | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 3 E0 | 14/08/10 | Chelsea | West Brom | 6.0 | 0.0 | H | 2.0 | 0.0 | H | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 4 E0 | 14/08/10 | Sunderland | Birmingham | 2.0 | 2.0 | D | 1.0 | 0.0 | H | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 4418 E0 | 23/05/2021 | Liverpool | Crystal Palace | 2.0 | 0.0 | H | 1.0 | 0.0 | H | ... | 3.49 | -2.25 | 1.86 | 2.04 | 1.88 | 2.03 | 1.98 | 2.14 | 1.88 | 2.00 | . 4419 E0 | 23/05/2021 | Man City | Everton | 5.0 | 0.0 | H | 2.0 | 0.0 | H | ... | 2.77 | -1.75 | 2.01 | 1.89 | 1.99 | 1.89 | 2.20 | 2.00 | 2.03 | 1.85 | . 4420 E0 | 23/05/2021 | Sheffield United | Burnley | 1.0 | 0.0 | H | 1.0 | 0.0 | H | ... | 2.05 | 0.00 | 2.04 | 1.86 | 2.05 | 1.86 | 2.17 | 1.90 | 2.03 | 1.84 | . 4421 E0 | 23/05/2021 | West Ham | Southampton | 3.0 | 0.0 | H | 2.0 | 0.0 | H | ... | 2.14 | -0.75 | 2.00 | 1.90 | 2.02 | 1.91 | 2.06 | 2.01 | 1.99 | 1.89 | . 4422 E0 | 23/05/2021 | Wolves | Man United | 1.0 | 2.0 | A | 1.0 | 2.0 | A | ... | 1.62 | -0.25 | 2.04 | 1.86 | 2.10 | 1.84 | 2.10 | 1.94 | 2.00 | 1.88 | . 4423 rows × 139 columns . The data we require is already processed and presented in the FTR column. All we need to do is to count the occurrences and then plot a graph. . #count the occurrences of each outcome as a percentage of all games game_counts = footdata[&quot;FTR&quot;].value_counts() game_counts . H 1967 A 1359 D 1096 Name: FTR, dtype: int64 . That gives us the numbers of games that ended as H - Home win, A - Away win, and D Draw in the English Premier League between 2010 and 2021. . Now to plot the bar chart. To make it more meaningful first, extract the number of games as a percentage then plot the graph. . #plot the data in a bar chart after converting to percentage gamedata = footdata.FTR percent = gamedata.value_counts(normalize=True).mul(100).round(1).astype(str) + &#39;%&#39; gamedata.value_counts(normalize=True).mul(100).round(1).sort_values(ascending=False).plot(kind = &#39;bar&#39;) plt.title(&#39;Outcome of Englsih Premier League Games 2010 to 2021&#39;) plt.ylabel(&#39;Percentage&#39;) plt.xlabel(&#39;Outcome&#39;) plt.annotate(&#39;[Source:footballdata.co.uk]&#39;, (0,0), (0,-30), fontsize=10, xycoords=&#39;axes fraction&#39;, textcoords=&#39;offset points&#39;, va=&#39;top&#39;) percent . H 44.5% A 30.7% D 24.8% Name: FTR, dtype: object . So there is a start. We now know that around 75% of games end with a result while just under 25% end in a draw. Armed with that information, then we can make informed decisions. While that information may be useful, in itself is not sufficient to begin to construct a profitable strategy or predict outcomes to any degree of certainty. We need to delve deeper. . And so the journey begins, next step . . . .",
            "url": "https://alyferg.github.io/data-science-adventures/2021/11/14/Application.html",
            "relUrl": "/2021/11/14/Application.html",
            "date": " • Nov 14, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Resources",
            "content": "It is important to have the right resources to hand to make the whole job lot easier. Perhaps the most important of these for the novice is the training resources. There is a huge volume of information and also a raft of other sites, blogs, and tools that can make the whole process more straightforward. . The initial choice when researching information or learning about Machine Learning is either utilising blogs or YouTube. Personally, I prefer to use YouTube as I find that a lot easier to follow and can also see the code and the results on the screen. That&#39;s not to say that I don&#39;t use blogs that can provide a useful depth that perhaps YouTube doesn&#39;t and quite often the blogs such as my own include blocks of code that you can copy to try out for yourself. . The volume of videos on YouTube is incredible and many of them are really long. There are some that contain a full 12-hour course and 3 to 4-hour videos are not uncommon. The beauty of many of these videos is that they are properly indexed with chapter markings though some of the older ones don&#39;t. Many of the video descriptions also provide links to the code either on github or a website that can be downloaded to follow along. It is truly amazing the volume and quality of the content available on YouTube on the subject of Machine Learning. This is a testament to the community that is prepared to put so much time and effort into disseminating their knowledge. . Here are some of the useful channels that I have discovered on YouTube. This is not intended as an exhaustive list or a recommendation. I can say is that I have found these particular channels very useful in gaining the understanding I now have of the subject. There are many many more. . Simplilearn Siraj Raval 3blue1brown edeureka Tech with Tim . This is perhaps the most comprehensive one I found about deep learning it is a full course and very worthy of your attention if you are serious about learning more. . Practical Deep Learning for Coders . In the same vain here are a number of blogs/websites that I am found useful easy for following particular subjects or looking up one particular Key area. . stackoverflow a great source of wisdom and answers on many subjects. freecodecamp.org some great articles, I particularly like the flow of many of them. Python Data Science Handbook comprehensive and structured list of Python Data Science topics. pythonguides plenty of help in Python with examples. kite another very detailed list of useful code and topics. . There are many many more available, I am sure you can find your own. . I am sure that there are many different tools out there that people will find particularly useful depending on their exact requirements. Here are two that I have found particularly useful. . Grepper . Githib . Finally, for comprehensive access to many publicly available datasets to play, test, and learn from them look at Kaggle. They also run competitions from time to time to help hone your skills. . Kaggle . For the Data Science example in this blog I have chosen the &quot;modeschoice&quot; dataset. The statsmodels explanation of this dataset is; . The data, collected as part of a 1987 intercity mode choice study, are a sub-sample of 210 non-business trips between Sydney, Canberra and Melbourne in which the traveler chooses a mode from four alternatives (plane, car, bus and train). . In this example we will be looking at &quot;Terminal Time&quot; i.e. the time waiting at the terminal for the chosen mode of transport. As the assumption of the survey is that when using a car there is zero waiting time. Therefore, we will be charting the average wait time of Air, Rail and Bus Transport. . # load dependencies and data import pandas as pd import numpy as np import statsmodels.api as sm from matplotlib import pyplot as plt modechoice = sm.datasets.modechoice df = modechoice.load_pandas().data #check that the data is loaded and see how it looks df . individual mode choice ttme invc invt gc hinc psize . 0 1.0 | 1.0 | 0.0 | 69.0 | 59.0 | 100.0 | 70.0 | 35.0 | 1.0 | . 1 1.0 | 2.0 | 0.0 | 34.0 | 31.0 | 372.0 | 71.0 | 35.0 | 1.0 | . 2 1.0 | 3.0 | 0.0 | 35.0 | 25.0 | 417.0 | 70.0 | 35.0 | 1.0 | . 3 1.0 | 4.0 | 1.0 | 0.0 | 10.0 | 180.0 | 30.0 | 35.0 | 1.0 | . 4 2.0 | 1.0 | 0.0 | 64.0 | 58.0 | 68.0 | 68.0 | 30.0 | 2.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 835 209.0 | 4.0 | 1.0 | 0.0 | 27.0 | 510.0 | 82.0 | 20.0 | 1.0 | . 836 210.0 | 1.0 | 0.0 | 64.0 | 66.0 | 140.0 | 87.0 | 70.0 | 4.0 | . 837 210.0 | 2.0 | 0.0 | 44.0 | 54.0 | 670.0 | 156.0 | 70.0 | 4.0 | . 838 210.0 | 3.0 | 0.0 | 53.0 | 33.0 | 664.0 | 134.0 | 70.0 | 4.0 | . 839 210.0 | 4.0 | 1.0 | 0.0 | 12.0 | 540.0 | 94.0 | 70.0 | 4.0 | . 840 rows × 9 columns . #we will be grouping the Terminal Time column by mode and calculating an average #this gives us the average wait time for each mode air = df.groupby(&#39;mode&#39;)[&#39;ttme&#39;].mean() #view the table air . mode 1.0 61.009524 2.0 35.690476 3.0 41.657143 4.0 0.000000 Name: ttme, dtype: float64 . # add labels to data labels = [&#39;Plane - 61&#39;,&#39;Train - 35&#39;,&#39;Bus - 41&#39;, &#39;Car - 0&#39;] # plot the pie chart plot = air.plot.pie(labeldistance=None, autopct=&#39;%1.1f%%&#39;,figsize=(10, 10)) plt.legend(loc=&#39;best&#39;,title=&#39;Average wait time (mins)&#39;,title_fontsize=&#39;large&#39;,labels=labels) . &lt;matplotlib.legend.Legend at 0x7fd4b50e6940&gt; . Therefore, if you took an equal number of trips via Air, Rail and Bus then you would spend 44.1% of your waiting time waiting at the Airport, 25.8% at the Rail Station and 30.1% at the Bus Station. . The legend shows the average wait time for each. .",
            "url": "https://alyferg.github.io/data-science-adventures/2021/10/28/Resources.html",
            "relUrl": "/2021/10/28/Resources.html",
            "date": " • Oct 28, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Environment",
            "content": "Having the correct environment for running your Python code is very important. I have found that having two environments to run code can be beneficial. The first of these is a machine resident version and the second is online with an IDE. . My normal machine is a Chromebook running a Linux container that is ideal for the majority of software around today. For Python I have created an Anaconda environment within the Linux container which enables me to run either Jupyter notebooks or Spyder as a Python development environment. I did start by running Spyder probably because I&#39;m still a little bit old school and like to see things and know where they are. Spyder gives you the option to interrogate variables and tables which can be useful. However, I have now started using Jupyter notebooks almost exclusively. The flexibility they provide enables you to work in a more structured way and also make plenty of notes as you go along.&quot; . When I say that I am a little bit old school in many ways that is very true. I did start programming originally in Basic if anyone remembers this, in fact at one time I was uploading Hex instructions into microprocessors to make them do things. Moving on from Basic then I did some programming in Pascal, Cobol, and several other languages. The more modern languages have generally escaped me with the exception of a brief detour into Java several years ago. This was mainly due to my work arrangements rather than a lack of interest in pursuing a programming career. . Anyway, back to the environment. The Linux environment is ideal for me as I do travel from time to time and so don&#39;t always have access to online facilities. It is perfect for running smaller machine learning operations such as regression or decision trees. It also gives you control over the environment and is available whenever needed. . If you do intend to undertake any form of Deep Learning though, then you will probably need an online IDE. I say this because the power required to undertake Deep Learning tasks will require access to GPU computing power. That is unless you are lucky enough to have the hardware at home, My choice of IDE is Gradient which is free and powerful but can be hard to connect at busy times. Gradient can be found here . So, armed with your environments whether single or double, local or remote, then you are good to go. . This time we will do some analysis on another well-known dataset, the Boston Housing Dataset. . #import dependencies and data import numpy as np import matplotlib.pyplot as plt import pandas as pd import seaborn as sns %matplotlib inline from sklearn.datasets import load_boston boston = load_boston() df = pd.DataFrame(boston.data).head() # drop a column of zero entries df.drop(3,1, inplace = True) #let&#39;s also have a look at the data df.head() . /tmp/ipykernel_10189/410762870.py:13: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument &#39;labels&#39; will be keyword-only df.drop(3,1, inplace = True) . 0 1 2 4 5 6 7 8 9 10 11 12 . 0 0.00632 | 18.0 | 2.31 | 0.538 | 6.575 | 65.2 | 4.0900 | 1.0 | 296.0 | 15.3 | 396.90 | 4.98 | . 1 0.02731 | 0.0 | 7.07 | 0.469 | 6.421 | 78.9 | 4.9671 | 2.0 | 242.0 | 17.8 | 396.90 | 9.14 | . 2 0.02729 | 0.0 | 7.07 | 0.469 | 7.185 | 61.1 | 4.9671 | 2.0 | 242.0 | 17.8 | 392.83 | 4.03 | . 3 0.03237 | 0.0 | 2.18 | 0.458 | 6.998 | 45.8 | 6.0622 | 3.0 | 222.0 | 18.7 | 394.63 | 2.94 | . 4 0.06905 | 0.0 | 2.18 | 0.458 | 7.147 | 54.2 | 6.0622 | 3.0 | 222.0 | 18.7 | 396.90 | 5.33 | . #this time we will print out a correlation matrix with a heatmap of the dataset n&quot;, correlation = df.corr().round(3) plt.figure(figsize = (15,10)) sns.heatmap(data=correlation, annot=True) . &lt;AxesSubplot:&gt; . A heat-map such as this is often the first step in looking for a correlation between columns in a dataset. The level of correlation is shown by the figure in each square with 1 being 100% and 0 being 0%. The colours give a useful quick visual guide. The lighter the better and the darker the less or negative correlation. . Of course, this leads me to one final and very important point. One thing that has become very evident quickly is the importance of looking at the data and just as important visualising it in some way. Whether this is with graphs, scatter diagrams or heat-maps it&#39;s vital to have an understanding of how the data looks and visualisations of this type are a good way to do it. .",
            "url": "https://alyferg.github.io/data-science-adventures/2021/10/26/Environment.html",
            "relUrl": "/2021/10/26/Environment.html",
            "date": " • Oct 26, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Begining",
            "content": "1 month ago I only knew that Python did actually exist, but had never written a line of code for it. Similarly, whilst the prospect of understanding Data Science was something that I would find interesting, the idea of actually doing anything with it seemed remote. Then, I discovered an application I wanted to follow through, one for which Data Science seemed to be the ideal solution. And so began a journey that even in a short space of time has taken me so far. . There are a number of things that I have learned very quickly. . that the internet is awash with resources to support anyone wishing to learn about python and Data Science. So much so that sometimes it&#39;s difficult to know when to leave off and start being practical. | I have been truly amazed at the number and quality of data sets that are freely available out there to be used either for real-world calculations or for testing to develop systems. | that it really is not as difficult as you may think and even within a short space of time you can be producing meaningful results from the data we all have access to. | So, as much as a journal for me rather than being a mechanism for passing on any wisdom a Blog seemed to be the ideal way to go. This blog is built using Jupyter notebooks that are turned into blog posts automatically by Fastpages via github. More about that later. . I don&#39;t think any blog on the subject of Data Science especially one written in a Jupyter notebook would be complete without some kind of code and graphical representation. . There are numerous datasets available on websites and indeed within Python modules and libraries. Soem are very well-known such as this one. The Iris dataset. We will load it and plot a scatter diagram of the columns to see if there is any correlation between them. . #let&#39;s start by loading a dataset #here we will load the famous Iris dataset after loading modules import matplotlib.pyplot as plt import pandas as pd import numpy as np from sklearn import datasets from pandas.plotting import scatter_matrix #load the iris dataset iris = datasets.load_iris() df = pd.DataFrame(iris.data, columns=iris.feature_names) #let&#39;s have a quick check on the data df.head() . sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) . 0 5.1 | 3.5 | 1.4 | 0.2 | . 1 4.9 | 3.0 | 1.4 | 0.2 | . 2 4.7 | 3.2 | 1.3 | 0.2 | . 3 4.6 | 3.1 | 1.5 | 0.2 | . 4 5.0 | 3.6 | 1.4 | 0.2 | . #this will plot a scatter plot of each column against every other to look for a correlation scatter_matrix(df, figsize=(10,10)) . array([[&lt;AxesSubplot:xlabel=&#39;sepal length (cm)&#39;, ylabel=&#39;sepal length (cm)&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;sepal width (cm)&#39;, ylabel=&#39;sepal length (cm)&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;petal length (cm)&#39;, ylabel=&#39;sepal length (cm)&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;petal width (cm)&#39;, ylabel=&#39;sepal length (cm)&#39;&gt;], [&lt;AxesSubplot:xlabel=&#39;sepal length (cm)&#39;, ylabel=&#39;sepal width (cm)&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;sepal width (cm)&#39;, ylabel=&#39;sepal width (cm)&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;petal length (cm)&#39;, ylabel=&#39;sepal width (cm)&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;petal width (cm)&#39;, ylabel=&#39;sepal width (cm)&#39;&gt;], [&lt;AxesSubplot:xlabel=&#39;sepal length (cm)&#39;, ylabel=&#39;petal length (cm)&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;sepal width (cm)&#39;, ylabel=&#39;petal length (cm)&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;petal length (cm)&#39;, ylabel=&#39;petal length (cm)&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;petal width (cm)&#39;, ylabel=&#39;petal length (cm)&#39;&gt;], [&lt;AxesSubplot:xlabel=&#39;sepal length (cm)&#39;, ylabel=&#39;petal width (cm)&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;sepal width (cm)&#39;, ylabel=&#39;petal width (cm)&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;petal length (cm)&#39;, ylabel=&#39;petal width (cm)&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;petal width (cm)&#39;, ylabel=&#39;petal width (cm)&#39;&gt;]], dtype=object) . You can draw your own conclusions from the data. The scatter matrix shows the relationship between the columns of the data. Once you have seen this then you may wish to view another data representation or look for a more detailed view of any specific correlation you see. . So, a very simple start, but I&#39;ve learned a lot more in the last month. This is where I started out and I must admit I&#39;m beginning to enjoy the journey immensely. At the moment I really still don&#39;t know what I don&#39;t know, but I&#39;m enjoying finding out what I don&#39;t know and then filling the gaps. .",
            "url": "https://alyferg.github.io/data-science-adventures/2021/10/25/The_Beginning.html",
            "relUrl": "/2021/10/25/The_Beginning.html",
            "date": " • Oct 25, 2021"
        }
        
    
  

  
  
      ,"page0": {
          "title": "An Example Markdown Post",
          "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
          "url": "https://alyferg.github.io/data-science-adventures/images/2020-01-14-test-markdown-post.html",
          "relUrl": "/images/2020-01-14-test-markdown-post.html",
          "date": ""
      }
      
  

  

  
      ,"page2": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . I have worked with data for all my life. In the early days of course, I didn’t recognise that it was data as such. As someone who was brought up pre-computer (personal, that is though business computers at the time were mainframes) this tended to be in paper format. Even now I can remember my teenage self pouring over maps for hours on end, studying the smallest detail on timetables, or wading through page after page of books filled with information. The latter tended to be by lists of things such as Kings, Countries, Animals, etc along with their important statistical information. These books were the forerunners of what are now coffee table tombes displayed to impress visitors. . As my career developed, then I found I was often in the situation of collecting, collating, and presenting data. Again, much of this tended to be in list form covering either specific locations or events. The huge volumes of data that are available today on even the minutest details were just unheard of at the time. . Computing did gradually become more and more important to businesses. As business processes developed around the use of data that increasingly tended to be more in spreadsheet format being involved was a natural progression for me. One of the talents that I did develop as things progressed was the ability to move data from one system to another. Due to a general level of incompatibility between systems in the early days, this orphan involved transposing data somewhere to extract the relevant fields. It was fun and always lead to a sense of achievement once you are able to get the data into the target format required. . Coming from an engineering background I often did get involved in analysing data. Though it was only when I got involved in the planning and business side of operations that I began to work with larger volumes of data. My career path did digress from the route that Data Science was taking and I gradually moved further and further away from it. It is only in the very recent past that I’ve begun to get reacquainted with the beauties of dealing with data. . I have to say that things are much easier now than they were in the early days. Well, perhaps that isn’t strictly true. It certainly is much easier to manipulate transpose and move data around than it ever was. Of course, the challenge now is that the volumes of data being collected for analysis are so much more than they ever were that data overload is a realistic possibility. It is also true that as Computing has developed, the power of machines has grown at an amazing rate. This has resulted in the fact that today the computing power available even in the home on a basic machine is as powerful as you could wish for while undertaking basic data analysis. . Therefore, I am really looking forward to my journey from an old data manipulator to a modern Data Science professional. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://alyferg.github.io/data-science-adventures/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://alyferg.github.io/data-science-adventures/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}