{
  
    
        "post0": {
            "title": "All at odds - part 1",
            "content": "It won&#39;t come as a shock to anyone that this is not the only blog on this particular subject. There are many blogs YouTube channels, etc. on the subject of data Science and specifically data Science in Sport and Sports betting. It is interesting when looking at these other resources how different people take different routes, one assumes or at least talks, with the same destination in mind. To date, I have not seen another blog that uses the same starting point as I am about to, though there is a very good chance that there is one out there somewhere. . Historically, the only place to go to put a bet on sports was a bookmaker, in fact until the late 1990s that would have meant visiting a shop for most people. At a bookmaker, you could place a back bet on a particular outcome and that was it. Now, when you visit a bookmaker you have a huge range of different options available to you. Especially in sports like football all where you can bet on everything from the outcome of the match to the number of corners a particular side will have. That is not the only thing that changed in Sports betting because around 2000 betting exchanges were introduced. Exchanges are effectively peer-to-peer betting services where the exchange provides the infrastructure to allow people to exchange bets. Your exchange rate could be against another individual, a syndicate, an organisation, or even a bookmaker, you just don&#39;t know as you&#39;ll never see the other side of the bet. The other thing that changed with the advent of betting exchanges was that you can now bet on something not happening, i.e. you can bet against a particular team winning rather than just backing them to win. . My journey into the analysis of Sports Betting will begin with looking at the accuracy of the odds at the beginning of a particular event. The odds offered by individual bookmakers tend to match closely those offered by other bookmakers and those offered at the exchanges. This needs to be the case otherwise there would be the potential for bettors to exploit differences in prices between different bookmakers and exchanges. That technique is known as arbitrage betting. The price that is available just before the commencement of a particular event it&#39;s probably as close as the real probability of a particular outcome for that event, for two very good reasons. Where the price is being set by pressure from an exchange, then we have the &quot;wisdom of the crowd&quot; coming into play. There could be thousands of individuals looking for value that will drive the price to a balance point of maximum value. The second reason, where the price is being driven by pressure from bookmakers then their experience will come into play. This knowledge over many many years, plus, no doubt, the data scientists that they employ to calculate real probability, leads to the price at that particular point in time will be as close to real as possible. . Therefore, my opening question is . . . . How accurate are the starting odds set for a market in a particular event? . I will be using the starting prices from a bookmaker for this particular exercise, but that&#39;s only due to the availability of the data and is not a reflection of the perceived accuracy of that data. Should data from betting exchanges for the same event start to be collected then there will be very little difference between not and the data we have used. The first analysis will be to look at match odds data, i.e. Home win away win what draws in Premier League football matches between 2010 and 2021. . The analysis is undertaken with the data used in the previous set downloaded from . FootballData.co.uk . First with the English Premiership between 2010 and 2021, then we can look at other leagues. . # load dependencies and data import pandas as pd import numpy as np import glob import matplotlib.pyplot as plt import seaborn as sns from pandas.plotting import scatter_matrix #load all csv files and append to a single dataframe path = &#39;../../../GitHub/England-EPL&#39; all_files = glob.glob(path + &quot;/*.csv&quot;) li = [] for filename in all_files: df = pd.read_csv(filename, index_col=None, header=0) li.append(df) footdata = pd.concat(li, axis=0, ignore_index=True) footdata.rename(columns={&#39;B365H&#39;: &#39;Home odds&#39;, &#39;B365A&#39;: &#39;Away odds&#39;, &#39;B365D&#39;: &#39;Draw odds&#39;}, inplace=True, errors=&#39;raise&#39;) #check that the data is loaded and see how it looks footdata . Div Date HomeTeam AwayTeam FTHG FTAG FTR HTHG HTAG HTR ... AvgC&lt;2.5 AHCh B365CAHH B365CAHA PCAHH PCAHA MaxCAHH MaxCAHA AvgCAHH AvgCAHA . 0 E0 | 14/08/10 | Aston Villa | West Ham | 3.0 | 0.0 | H | 2.0 | 0.0 | H | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 1 E0 | 14/08/10 | Blackburn | Everton | 1.0 | 0.0 | H | 1.0 | 0.0 | H | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2 E0 | 14/08/10 | Bolton | Fulham | 0.0 | 0.0 | D | 0.0 | 0.0 | D | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 3 E0 | 14/08/10 | Chelsea | West Brom | 6.0 | 0.0 | H | 2.0 | 0.0 | H | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 4 E0 | 14/08/10 | Sunderland | Birmingham | 2.0 | 2.0 | D | 1.0 | 0.0 | H | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 4418 E0 | 23/05/2021 | Liverpool | Crystal Palace | 2.0 | 0.0 | H | 1.0 | 0.0 | H | ... | 3.49 | -2.25 | 1.86 | 2.04 | 1.88 | 2.03 | 1.98 | 2.14 | 1.88 | 2.00 | . 4419 E0 | 23/05/2021 | Man City | Everton | 5.0 | 0.0 | H | 2.0 | 0.0 | H | ... | 2.77 | -1.75 | 2.01 | 1.89 | 1.99 | 1.89 | 2.20 | 2.00 | 2.03 | 1.85 | . 4420 E0 | 23/05/2021 | Sheffield United | Burnley | 1.0 | 0.0 | H | 1.0 | 0.0 | H | ... | 2.05 | 0.00 | 2.04 | 1.86 | 2.05 | 1.86 | 2.17 | 1.90 | 2.03 | 1.84 | . 4421 E0 | 23/05/2021 | West Ham | Southampton | 3.0 | 0.0 | H | 2.0 | 0.0 | H | ... | 2.14 | -0.75 | 2.00 | 1.90 | 2.02 | 1.91 | 2.06 | 2.01 | 1.99 | 1.89 | . 4422 E0 | 23/05/2021 | Wolves | Man United | 1.0 | 2.0 | A | 1.0 | 2.0 | A | ... | 1.62 | -0.25 | 2.04 | 1.86 | 2.10 | 1.84 | 2.10 | 1.94 | 2.00 | 1.88 | . 4423 rows × 139 columns . # to make the dataframe more manageable and readable let&#39;s select only the columns we want to analyse footdata_odds = footdata[[&#39;FTR&#39;,&#39;Home odds&#39;,&#39;Away odds&#39;,&#39;Draw odds&#39;]] footdata_odds . FTR Home odds Away odds Draw odds . 0 H | 2.00 | 4.0 | 3.30 | . 1 H | 2.88 | 2.5 | 3.25 | . 2 D | 2.20 | 3.4 | 3.30 | . 3 H | 1.17 | 17.0 | 7.00 | . 4 D | 2.10 | 3.6 | 3.30 | . ... ... | ... | ... | ... | . 4418 H | 1.14 | 15.0 | 8.50 | . 4419 H | 1.44 | 6.5 | 5.00 | . 4420 H | 2.80 | 2.4 | 3.60 | . 4421 H | 1.65 | 5.0 | 4.00 | . 4422 A | 2.50 | 2.7 | 3.50 | . 4423 rows × 4 columns . # there are some NaN values in the dataframe. Here are the counts per column footdata_odds.isna().sum() . FTR 1 Home odds 1 Away odds 1 Draw odds 1 dtype: int64 . We will initially look at this data via a Boxplot. Boxplot is similar to a candle chart for anyone who has done any technical charting. What a Boxplot does is give us a Box showing where the majority of the data is while showing outliers as lines and dots. In this way, we can see where the majority of the data sits. . The main Box in a Boxplot shows all data from the Q1 or 25th percentile to the Q3 or 75th percentile. Using a Boxplot we can see how closely grouped the data or in this case the starting odds are. The lines and dots above and below these levels will show how spread out the rest of the data is. With this technique, we will be able to get a good picture of where the starting odds are in relation to each outcome and thus an idea of the accuracy of the odds. . We will look at 3 Boxplots - one for each of the 3 possible outcomes. To do this we will split the odds dataset we just created into 3. . #create a table with a subset of the data for Home win, Away win and Draw footdata_odds_h = footdata_odds.loc[footdata_odds[&#39;FTR&#39;] == &quot;H&quot;] footdata_odds_a = footdata_odds.loc[footdata_odds[&#39;FTR&#39;] == &quot;A&quot;] footdata_odds_d = footdata_odds.loc[footdata_odds[&#39;FTR&#39;] == &quot;D&quot;] . #plot the boxplot for games that ended a home win. Nice and big, so we can get a good look at the data. footdata_odds_h.plot(kind=&#39;box&#39;, figsize=(12,15),title=(&#39;Starting odds for home wins&#39;),grid=&#39;TRUE&#39;) . &lt;AxesSubplot:title={&#39;center&#39;:&#39;Starting odds for home wins&#39;}&gt; . Now we know what a Boxplot looks like then let&#39;s have a more detailed explanation of what the elements are. Looking first at the box, the horizontal line in the middle represents the 50% point of the data numbers, the median. From the median line to the bottom of the box is 50% to 25% of data points, while the top part of the box is 50% to 75% of data points. The box, i.e. 25% to 75% of data points is known as the interquartile range. . The lines emanating from the top and bottom of the boxes are known as whiskers and represent the top (75%+) and bottom (25%-) of the data points. The bar across the whiskers is set at (1.5 * Interquartile Range) and the dots above and below these are the Outliers. The Outliers will generally cover approx 0.7% of data values, i.e. 0.35% highest and lowest values in the range. . With the above in mind, how to interpret the Boxplot. The obvious feature is that for Home wins the vast majority of Home odds fall below those of the Away win and the Draw. This is the first area of further identification that we have identified. Before we do though, let&#39;s have a look at the same plots for Away wins and Draws as the outcome. . #let&#39;s plot the boxplot for games that ended as an away win. footdata_odds_a.plot(kind=&#39;box&#39;,figsize=(12,15),title=(&#39;Starting odds for away wins&#39;),grid=&#39;TRUE&#39;) . &lt;AxesSubplot:title={&#39;center&#39;:&#39;Starting odds for away wins&#39;}&gt; . #let&#39;In the Boxplots for Away wins and Draws the overlap of odds is much more pronounced than for the Home wins we looked at earlier. It would be fair to say at this stage that for Away wins and Draws odds alone would not be a sufficient distinguishing factor to develop any conclusions from. Another variable would be required to provide clearer results. That leaves us with the further analysis of the Home win odds in relation to Home win outcomes.s plot the boxplot for games that ended a home win. Nice and big, so we can see the outcome. footdata_odds_d.plot(kind=&#39;box&#39;,figsize=(12,15),title=(&#39;Starting odds for draws&#39;),grid=&#39;TRUE&#39;) . &lt;AxesSubplot:title={&#39;center&#39;:&#39;Starting odds for draws&#39;}&gt; . In the Boxplots for Away wins and Draws the overlap of odds is much more pronounced than for the Home wins we looked at earlier. It would be fair to say at this stage that for Away wins and Draws odds alone would not be a sufficient distinguishing factor to develop any conclusions from. Another variable would be required to provide clearer results. That leaves us with the further analysis of the Home win odds in relation to Home win outcomes. . That will be our starting point for part 2 of this series on using starting odds for analysis of outcome. .",
            "url": "https://alyferg.github.io/data-science-adventures/2021/11/24/All-at-odds-Part-1.html",
            "relUrl": "/2021/11/24/All-at-odds-Part-1.html",
            "date": " • Nov 24, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Application",
            "content": "I mentioned in an earlier post that one of the drivers behind my interest in learning Data Science/Machine Learning was that I had an application. While I see this application as a driver to learn and explore the data, it is the motivation rather than the outcome I am seeking. So here I want to briefly run over the application to explain what I want to achieve before beginning the journey in vain. . The application that I wish to develop is around Sports Betting and/or Trading. I don&#39;t want this blog to become a detailed description of Sports Betting. Therefore, if you wish to know more about Betting and Trading Sports then I suggest you search for some of the numerous resources available. Specifically, I am interested in predicting the outcomes of Football (that&#39;s in the British context) matches and to a lesser extent horse races. A tall order. . It will be worth explaining at this point what this journey is not about. It is not about &quot;beating the Bookies,&quot; not about &quot;making money,&quot; not about finding &quot;a Holy Grail,&quot; in Sports Betting. What I hope to do is to explore the data to find patterns and to be able to predict outcomes of Football, for example, with a reasonable degree of accuracy. These predictions can then be used in further investigation. One thing I have learned in Sports Betting is that there is no such thing as a &quot;certainty&quot;. Any individual, team or horse can have an &quot;off&quot; day, be affected by the conditions, be over-confident etc. etc. etc. All of which can lead to an unexpected outcome, so having a shortlist to consider on the day is the sensible way to go. . So that is the Application. Some of the data I will be using is in the public domain, other data is proprietary and cannot be shared. . My intention is to follow various techniques, i.e. Numerical Regression, Logical Regression, Decision Trees, Random Forests, Deep Learning and see where we get. There may be others that pop up on the way. . I have no idea at the moment how long the journey may be and where it will take me. Isn&#39;t that exciting. . As we now know the path we are following then let&#39;s start with a basic analysis of some of the data we will be using. In a Football Match, there are three possible outcomes (unless it is a cup game that does require a winner). What proportion of games ends up with each of the three possible outcomes? The data was downloaded from . FootballData.co.uk . The data is stored in csv files on my computer with 1 file per season. The first cells will load the dependencies and all csv files and append them to a single dataframe. . The data loaded is for English Premier League from 2010 to 2021. . # load dependencies and data import pandas as pd import numpy as np import glob from matplotlib import pyplot as plt import seaborn as sns #load all csv files and append to a single dataframe path = &#39;../../../GitHub/England-EPL&#39; all_files = glob.glob(path + &quot;/*.csv&quot;) li = [] for filename in all_files: df = pd.read_csv(filename, index_col=None, header=0) li.append(df) footdata = pd.concat(li, axis=0, ignore_index=True) #check that the data is loaded and see how it looks footdata . Div Date HomeTeam AwayTeam FTHG FTAG FTR HTHG HTAG HTR ... AvgC&lt;2.5 AHCh B365CAHH B365CAHA PCAHH PCAHA MaxCAHH MaxCAHA AvgCAHH AvgCAHA . 0 E0 | 14/08/10 | Aston Villa | West Ham | 3.0 | 0.0 | H | 2.0 | 0.0 | H | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 1 E0 | 14/08/10 | Blackburn | Everton | 1.0 | 0.0 | H | 1.0 | 0.0 | H | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2 E0 | 14/08/10 | Bolton | Fulham | 0.0 | 0.0 | D | 0.0 | 0.0 | D | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 3 E0 | 14/08/10 | Chelsea | West Brom | 6.0 | 0.0 | H | 2.0 | 0.0 | H | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 4 E0 | 14/08/10 | Sunderland | Birmingham | 2.0 | 2.0 | D | 1.0 | 0.0 | H | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 4418 E0 | 23/05/2021 | Liverpool | Crystal Palace | 2.0 | 0.0 | H | 1.0 | 0.0 | H | ... | 3.49 | -2.25 | 1.86 | 2.04 | 1.88 | 2.03 | 1.98 | 2.14 | 1.88 | 2.00 | . 4419 E0 | 23/05/2021 | Man City | Everton | 5.0 | 0.0 | H | 2.0 | 0.0 | H | ... | 2.77 | -1.75 | 2.01 | 1.89 | 1.99 | 1.89 | 2.20 | 2.00 | 2.03 | 1.85 | . 4420 E0 | 23/05/2021 | Sheffield United | Burnley | 1.0 | 0.0 | H | 1.0 | 0.0 | H | ... | 2.05 | 0.00 | 2.04 | 1.86 | 2.05 | 1.86 | 2.17 | 1.90 | 2.03 | 1.84 | . 4421 E0 | 23/05/2021 | West Ham | Southampton | 3.0 | 0.0 | H | 2.0 | 0.0 | H | ... | 2.14 | -0.75 | 2.00 | 1.90 | 2.02 | 1.91 | 2.06 | 2.01 | 1.99 | 1.89 | . 4422 E0 | 23/05/2021 | Wolves | Man United | 1.0 | 2.0 | A | 1.0 | 2.0 | A | ... | 1.62 | -0.25 | 2.04 | 1.86 | 2.10 | 1.84 | 2.10 | 1.94 | 2.00 | 1.88 | . 4423 rows × 139 columns . The data we require is already processed and presented in the FTR column. All we need to do is to count the occurrences and then plot a graph. . #count the occurrences of each outcome as a percentage of all games game_counts = footdata[&quot;FTR&quot;].value_counts() game_counts . H 1967 A 1359 D 1096 Name: FTR, dtype: int64 . That gives us the numbers of games that ended as H - Home win, A - Away win, and D Draw in the English Premier League between 2010 and 2021. . Now to plot the bar chart. To make it more meaningful first, extract the number of games as a percentage then plot the graph. . #plot the data in a bar chart after converting to percentage gamedata = footdata.FTR percent = gamedata.value_counts(normalize=True).mul(100).round(1).astype(str) + &#39;%&#39; gamedata.value_counts(normalize=True).mul(100).round(1).sort_values(ascending=False).plot(kind = &#39;bar&#39;) plt.title(&#39;Outcome of Englsih Premier League Games 2010 to 2021&#39;) plt.ylabel(&#39;Percentage&#39;) plt.xlabel(&#39;Outcome&#39;) plt.annotate(&#39;[Source:footballdata.co.uk]&#39;, (0,0), (0,-30), fontsize=10, xycoords=&#39;axes fraction&#39;, textcoords=&#39;offset points&#39;, va=&#39;top&#39;) percent . H 44.5% A 30.7% D 24.8% Name: FTR, dtype: object . So there is a start. We now know that around 75% of games end with a result while just under 25% end in a draw. Armed with that information, then we can make informed decisions. While that information may be useful, in itself is not sufficient to begin to construct a profitable strategy or predict outcomes to any degree of certainty. We need to delve deeper. . And so the journey begins, next step . . . .",
            "url": "https://alyferg.github.io/data-science-adventures/2021/11/14/Application.html",
            "relUrl": "/2021/11/14/Application.html",
            "date": " • Nov 14, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Resources",
            "content": "It is important to have the right resources to hand to make the whole job lot easier. Perhaps the most important of these for the novice is the training resources. There is a huge volume of information and also a raft of other sites, blogs, and tools that can make the whole process more straightforward. . The initial choice when researching information or learning about Machine Learning is either utilising blogs or YouTube. Personally, I prefer to use YouTube as I find that a lot easier to follow and can also see the code and the results on the screen. That&#39;s not to say that I don&#39;t use blogs that can provide a useful depth that perhaps YouTube doesn&#39;t and quite often the blogs such as my own include blocks of code that you can copy to try out for yourself. . The volume of videos on YouTube is incredible and many of them are really long. There are some that contain a full 12-hour course and 3 to 4-hour videos are not uncommon. The beauty of many of these videos is that they are properly indexed with chapter markings though some of the older ones don&#39;t. Many of the video descriptions also provide links to the code either on github or a website that can be downloaded to follow along. It is truly amazing the volume and quality of the content available on YouTube on the subject of Machine Learning. This is a testament to the community that is prepared to put so much time and effort into disseminating their knowledge. . Here are some of the useful channels that I have discovered on YouTube. This is not intended as an exhaustive list or a recommendation. I can say is that I have found these particular channels very useful in gaining the understanding I now have of the subject. There are many many more. . Simplilearn Siraj Raval 3blue1brown edeureka Tech with Tim . This is perhaps the most comprehensive one I found about deep learning it is a full course and very worthy of your attention if you are serious about learning more. . Practical Deep Learning for Coders . In the same vain here are a number of blogs/websites that I am found useful easy for following particular subjects or looking up one particular Key area. . stackoverflow a great source of wisdom and answers on many subjects. freecodecamp.org some great articles, I particularly like the flow of many of them. Python Data Science Handbook comprehensive and structured list of Python Data Science topics. pythonguides plenty of help in Python with examples. kite another very detailed list of useful code and topics. . There are many many more available, I am sure you can find your own. . I am sure that there are many different tools out there that people will find particularly useful depending on their exact requirements. Here are two that I have found particularly useful. . Grepper . Githib . Finally, for comprehensive access to many publicly available datasets to play, test, and learn from them look at Kaggle. They also run competitions from time to time to help hone your skills. . Kaggle . For the Data Science example in this blog I have chosen the &quot;modeschoice&quot; dataset. The statsmodels explanation of this dataset is; . The data, collected as part of a 1987 intercity mode choice study, are a sub-sample of 210 non-business trips between Sydney, Canberra and Melbourne in which the traveler chooses a mode from four alternatives (plane, car, bus and train). . In this example we will be looking at &quot;Terminal Time&quot; i.e. the time waiting at the terminal for the chosen mode of transport. As the assumption of the survey is that when using a car there is zero waiting time. Therefore, we will be charting the average wait time of Air, Rail and Bus Transport. . # load dependencies and data import pandas as pd import numpy as np import statsmodels.api as sm from matplotlib import pyplot as plt modechoice = sm.datasets.modechoice df = modechoice.load_pandas().data #check that the data is loaded and see how it looks df . individual mode choice ttme invc invt gc hinc psize . 0 1.0 | 1.0 | 0.0 | 69.0 | 59.0 | 100.0 | 70.0 | 35.0 | 1.0 | . 1 1.0 | 2.0 | 0.0 | 34.0 | 31.0 | 372.0 | 71.0 | 35.0 | 1.0 | . 2 1.0 | 3.0 | 0.0 | 35.0 | 25.0 | 417.0 | 70.0 | 35.0 | 1.0 | . 3 1.0 | 4.0 | 1.0 | 0.0 | 10.0 | 180.0 | 30.0 | 35.0 | 1.0 | . 4 2.0 | 1.0 | 0.0 | 64.0 | 58.0 | 68.0 | 68.0 | 30.0 | 2.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 835 209.0 | 4.0 | 1.0 | 0.0 | 27.0 | 510.0 | 82.0 | 20.0 | 1.0 | . 836 210.0 | 1.0 | 0.0 | 64.0 | 66.0 | 140.0 | 87.0 | 70.0 | 4.0 | . 837 210.0 | 2.0 | 0.0 | 44.0 | 54.0 | 670.0 | 156.0 | 70.0 | 4.0 | . 838 210.0 | 3.0 | 0.0 | 53.0 | 33.0 | 664.0 | 134.0 | 70.0 | 4.0 | . 839 210.0 | 4.0 | 1.0 | 0.0 | 12.0 | 540.0 | 94.0 | 70.0 | 4.0 | . 840 rows × 9 columns . #we will be grouping the Terminal Time column by mode and calculating an average #this gives us the average wait time for each mode air = df.groupby(&#39;mode&#39;)[&#39;ttme&#39;].mean() #view the table air . mode 1.0 61.009524 2.0 35.690476 3.0 41.657143 4.0 0.000000 Name: ttme, dtype: float64 . # add labels to data labels = [&#39;Plane - 61&#39;,&#39;Train - 35&#39;,&#39;Bus - 41&#39;, &#39;Car - 0&#39;] # plot the pie chart plot = air.plot.pie(labeldistance=None, autopct=&#39;%1.1f%%&#39;,figsize=(10, 10)) plt.legend(loc=&#39;best&#39;,title=&#39;Average wait time (mins)&#39;,title_fontsize=&#39;large&#39;,labels=labels) . &lt;matplotlib.legend.Legend at 0x7fd4b50e6940&gt; . Therefore, if you took an equal number of trips via Air, Rail and Bus then you would spend 44.1% of your waiting time waiting at the Airport, 25.8% at the Rail Station and 30.1% at the Bus Station. . The legend shows the average wait time for each. .",
            "url": "https://alyferg.github.io/data-science-adventures/2021/10/28/Resources.html",
            "relUrl": "/2021/10/28/Resources.html",
            "date": " • Oct 28, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Environment",
            "content": "Having the correct environment for running your Python code is very important. I have found that having two environments to run code can be beneficial. The first of these is a machine resident version and the second is online with an IDE. . My normal machine is a Chromebook running a Linux container that is ideal for the majority of software around today. For Python I have created an Anaconda environment within the Linux container which enables me to run either Jupyter notebooks or Spyder as a Python development environment. I did start by running Spyder probably because I&#39;m still a little bit old school and like to see things and know where they are. Spyder gives you the option to interrogate variables and tables which can be useful. However, I have now started using Jupyter notebooks almost exclusively. The flexibility they provide enables you to work in a more structured way and also make plenty of notes as you go along.&quot; . When I say that I am a little bit old school in many ways that is very true. I did start programming originally in Basic if anyone remembers this, in fact at one time I was uploading Hex instructions into microprocessors to make them do things. Moving on from Basic then I did some programming in Pascal, Cobol, and several other languages. The more modern languages have generally escaped me with the exception of a brief detour into Java several years ago. This was mainly due to my work arrangements rather than a lack of interest in pursuing a programming career. . Anyway, back to the environment. The Linux environment is ideal for me as I do travel from time to time and so don&#39;t always have access to online facilities. It is perfect for running smaller machine learning operations such as regression or decision trees. It also gives you control over the environment and is available whenever needed. . If you do intend to undertake any form of Deep Learning though, then you will probably need an online IDE. I say this because the power required to undertake Deep Learning tasks will require access to GPU computing power. That is unless you are lucky enough to have the hardware at home, My choice of IDE is Gradient which is free and powerful but can be hard to connect at busy times. Gradient can be found here . So, armed with your environments whether single or double, local or remote, then you are good to go. . This time we will do some analysis on another well-known dataset, the Boston Housing Dataset. . #import dependencies and data import numpy as np import matplotlib.pyplot as plt import pandas as pd import seaborn as sns %matplotlib inline from sklearn.datasets import load_boston boston = load_boston() df = pd.DataFrame(boston.data).head() # drop a column of zero entries df.drop(3,1, inplace = True) #let&#39;s also have a look at the data df.head() . /tmp/ipykernel_10189/410762870.py:13: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument &#39;labels&#39; will be keyword-only df.drop(3,1, inplace = True) . 0 1 2 4 5 6 7 8 9 10 11 12 . 0 0.00632 | 18.0 | 2.31 | 0.538 | 6.575 | 65.2 | 4.0900 | 1.0 | 296.0 | 15.3 | 396.90 | 4.98 | . 1 0.02731 | 0.0 | 7.07 | 0.469 | 6.421 | 78.9 | 4.9671 | 2.0 | 242.0 | 17.8 | 396.90 | 9.14 | . 2 0.02729 | 0.0 | 7.07 | 0.469 | 7.185 | 61.1 | 4.9671 | 2.0 | 242.0 | 17.8 | 392.83 | 4.03 | . 3 0.03237 | 0.0 | 2.18 | 0.458 | 6.998 | 45.8 | 6.0622 | 3.0 | 222.0 | 18.7 | 394.63 | 2.94 | . 4 0.06905 | 0.0 | 2.18 | 0.458 | 7.147 | 54.2 | 6.0622 | 3.0 | 222.0 | 18.7 | 396.90 | 5.33 | . #this time we will print out a correlation matrix with a heatmap of the dataset n&quot;, correlation = df.corr().round(3) plt.figure(figsize = (15,10)) sns.heatmap(data=correlation, annot=True) . &lt;AxesSubplot:&gt; . A heat-map such as this is often the first step in looking for a correlation between columns in a dataset. The level of correlation is shown by the figure in each square with 1 being 100% and 0 being 0%. The colours give a useful quick visual guide. The lighter the better and the darker the less or negative correlation. . Of course, this leads me to one final and very important point. One thing that has become very evident quickly is the importance of looking at the data and just as important visualising it in some way. Whether this is with graphs, scatter diagrams or heat-maps it&#39;s vital to have an understanding of how the data looks and visualisations of this type are a good way to do it. .",
            "url": "https://alyferg.github.io/data-science-adventures/2021/10/26/Environment.html",
            "relUrl": "/2021/10/26/Environment.html",
            "date": " • Oct 26, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Begining",
            "content": "1 month ago I only knew that Python did actually exist, but had never written a line of code for it. Similarly, whilst the prospect of understanding Data Science was something that I would find interesting, the idea of actually doing anything with it seemed remote. Then, I discovered an application I wanted to follow through, one for which Data Science seemed to be the ideal solution. And so began a journey that even in a short space of time has taken me so far. . There are a number of things that I have learned very quickly. . that the internet is awash with resources to support anyone wishing to learn about python and Data Science. So much so that sometimes it&#39;s difficult to know when to leave off and start being practical. | I have been truly amazed at the number and quality of data sets that are freely available out there to be used either for real-world calculations or for testing to develop systems. | that it really is not as difficult as you may think and even within a short space of time you can be producing meaningful results from the data we all have access to. | So, as much as a journal for me rather than being a mechanism for passing on any wisdom a Blog seemed to be the ideal way to go. This blog is built using Jupyter notebooks that are turned into blog posts automatically by Fastpages via github. More about that later. . I don&#39;t think any blog on the subject of Data Science especially one written in a Jupyter notebook would be complete without some kind of code and graphical representation. . There are numerous datasets available on websites and indeed within Python modules and libraries. Soem are very well-known such as this one. The Iris dataset. We will load it and plot a scatter diagram of the columns to see if there is any correlation between them. . #let&#39;s start by loading a dataset #here we will load the famous Iris dataset after loading modules import matplotlib.pyplot as plt import pandas as pd import numpy as np from sklearn import datasets from pandas.plotting import scatter_matrix #load the iris dataset iris = datasets.load_iris() df = pd.DataFrame(iris.data, columns=iris.feature_names) #let&#39;s have a quick check on the data df.head() . sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) . 0 5.1 | 3.5 | 1.4 | 0.2 | . 1 4.9 | 3.0 | 1.4 | 0.2 | . 2 4.7 | 3.2 | 1.3 | 0.2 | . 3 4.6 | 3.1 | 1.5 | 0.2 | . 4 5.0 | 3.6 | 1.4 | 0.2 | . #this will plot a scatter plot of each column against every other to look for a correlation scatter_matrix(df, figsize=(10,10)) . array([[&lt;AxesSubplot:xlabel=&#39;sepal length (cm)&#39;, ylabel=&#39;sepal length (cm)&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;sepal width (cm)&#39;, ylabel=&#39;sepal length (cm)&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;petal length (cm)&#39;, ylabel=&#39;sepal length (cm)&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;petal width (cm)&#39;, ylabel=&#39;sepal length (cm)&#39;&gt;], [&lt;AxesSubplot:xlabel=&#39;sepal length (cm)&#39;, ylabel=&#39;sepal width (cm)&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;sepal width (cm)&#39;, ylabel=&#39;sepal width (cm)&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;petal length (cm)&#39;, ylabel=&#39;sepal width (cm)&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;petal width (cm)&#39;, ylabel=&#39;sepal width (cm)&#39;&gt;], [&lt;AxesSubplot:xlabel=&#39;sepal length (cm)&#39;, ylabel=&#39;petal length (cm)&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;sepal width (cm)&#39;, ylabel=&#39;petal length (cm)&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;petal length (cm)&#39;, ylabel=&#39;petal length (cm)&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;petal width (cm)&#39;, ylabel=&#39;petal length (cm)&#39;&gt;], [&lt;AxesSubplot:xlabel=&#39;sepal length (cm)&#39;, ylabel=&#39;petal width (cm)&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;sepal width (cm)&#39;, ylabel=&#39;petal width (cm)&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;petal length (cm)&#39;, ylabel=&#39;petal width (cm)&#39;&gt;, &lt;AxesSubplot:xlabel=&#39;petal width (cm)&#39;, ylabel=&#39;petal width (cm)&#39;&gt;]], dtype=object) . You can draw your own conclusions from the data. The scatter matrix shows the relationship between the columns of the data. Once you have seen this then you may wish to view another data representation or look for a more detailed view of any specific correlation you see. . So, a very simple start, but I&#39;ve learned a lot more in the last month. This is where I started out and I must admit I&#39;m beginning to enjoy the journey immensely. At the moment I really still don&#39;t know what I don&#39;t know, but I&#39;m enjoying finding out what I don&#39;t know and then filling the gaps. .",
            "url": "https://alyferg.github.io/data-science-adventures/2021/10/25/The_Beginning.html",
            "relUrl": "/2021/10/25/The_Beginning.html",
            "date": " • Oct 25, 2021"
        }
        
    
  

  
  
      ,"page0": {
          "title": "An Example Markdown Post",
          "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
          "url": "https://alyferg.github.io/data-science-adventures/2020-01-14-test-markdown-post.html",
          "relUrl": "/2020-01-14-test-markdown-post.html",
          "date": ""
      }
      
  

  

  
      ,"page2": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . This page is designed to write a little bit about me my background and explaining how I got to this point. All that will be coming up soon. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://alyferg.github.io/data-science-adventures/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://alyferg.github.io/data-science-adventures/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}